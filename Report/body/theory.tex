%Theory. Which theoretical tools did you use? 
\section{Theory}
To carry out the tasks of analysing this massive dataset we have applied different analysis and visualization techniques.
\subsection{Visualization}
is the discipline of creating diagrams that visually interpret data in such a way that the reader give the reader a higher level of abstraction.
\subsubsection{Histograms}
are a way to graphically represent data distribution of numerical data by dividing it in to non overlapping buckets, the first access represents the intervals and the second access represents the amount of cases that fall into that interval (bucket). In cases where the data is to divers it is customary instead use a logarithmic representation of one or both access.
\subsubsection{Network Graph}
are graphs used to represent relations between data, an example could be a family tree, social interactions or other relationships.
\subsection{Machine Learning}
is the discipline of collecting data and finding its principle components and by the process of supervised or unsupervised learning classify or regress the data.

\subsubsection{Bag of Words}
this is a model used for processing of natural language, in which you generate a vocabulary based on every different word in the raw text. This vocabulary can later be used for document classification. 

\subsubsection{Decision Trees}
is a tree like graph that stems from a root of the entire dataset and then on the basis of calculated "decisions" gets subdivided into new subsets, this goes on until the data is classified.

\subsection{Sentiment Analysis}
is the process of natural language processing using the each word in a files valance, arousal  and dominance scores to classify a page.

\subsubsection{Multinomial bayes' classifier} is a probabilistic classification method which is biassed upon Bayes' theorem. Using a multinomial model on samples thereby creating a feature vector used to calculate a probability of the given class. The sum of all the probabilities of the classes sum's up to one.